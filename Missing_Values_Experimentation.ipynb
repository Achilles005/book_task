{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273f12d1-376a-4b67-b43b-c6d24d2159fc",
   "metadata": {},
   "source": [
    "# Goal of these Experiments is to determine the best approach for missing value treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f8055c-5d9f-4f30-9695-c83a2b886877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Tokenizer, HashingTF, IDF\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import functions as F , SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6300d1-f9fd-468c-a29e-3f8ffccf465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mape(predictions, label_col=\"Impact\", prediction_col=\"prediction\"):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error (MAPE) on the predictions DataFrame.\"\"\"\n",
    "    mape = predictions.select(avg(abs((col(label_col) - col(prediction_col)) / col(label_col))).alias(\"mape\")).collect()[0][\"mape\"]\n",
    "    mape_percentage = mape * 100  # Convert to percentage\n",
    "    return mape_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e9f771-573f-4590-9324-9d273a3d967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MissingValueExperiment:\n",
    "    def __init__(self, df, experiment_name=\"Missing Value Treatment Experiments\"):\n",
    "        self.df = df\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        logger.info(\"Initialized MissingValueExperiment class.\")\n",
    "\n",
    "    def apply_missing_value_strategy(self, strategy):\n",
    "        \"\"\"Applies a missing value treatment strategy to the DataFrame.\"\"\"\n",
    "        logger.info(f\"Applying missing value strategy: {strategy}\")\n",
    "        if strategy == \"drop\":\n",
    "            return self.df.dropna(how=\"any\")\n",
    "        elif strategy == \"fill_empty\":\n",
    "            return self.df.fillna({\"Title\": \"\", \"description\": \"\", \"publishedDate\": \"\"})\n",
    "        elif strategy == \"fetch_api\":\n",
    "            return self.fetch_missing_data_from_api()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    def fetch_book_details_google_books(self, title, authors=None, max_retries=3, backoff_factor=2):\n",
    "        \"\"\"Fallback API: Fetch book details from Google Books API with retry and backoff.\"\"\"\n",
    "        API_KEY = \"AIzaSyBDVLGPYKHGQvL0q8TijDJPRPbhe1M5MrI\"  # Replace with your actual API key\n",
    "        API_URL = f'https://www.googleapis.com/books/v1/volumes?q=intitle:\"{title}\"&maxResults=1&key={API_KEY}'\n",
    "        if authors:\n",
    "            API_URL = f'https://www.googleapis.com/books/v1/volumes?q=intitle:\"{title}\"+inauthor:{authors}&maxResults=1&key={API_KEY}'\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(API_URL)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    if \"items\" in data:\n",
    "                        book_info = data[\"items\"][0][\"volumeInfo\"]\n",
    "                        logger.info(f\"Book details found for '{title}' using Google Books.\")\n",
    "                        \n",
    "                        # Delay after a successful API call\n",
    "                        time.sleep(5)\n",
    "                        \n",
    "                        # Format authors as a string in '[]' format\n",
    "                        formatted_authors = f\"[{', '.join(map(str, book_info.get('authors', [])))}]\"\n",
    "                        \n",
    "                        return {\n",
    "                            \"title\": str(title),\n",
    "                            \"authors\": str(formatted_authors),\n",
    "                            \"published_date\": str(book_info.get(\"publishedDate\", \"\")),\n",
    "                            \"description\": str(book_info.get(\"description\", \"\")),\n",
    "                            \"publisher\": str(book_info.get(\"publisher\", \"\"))\n",
    "                        }\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Error fetching book details from Google Books for '{title}': {e}\")\n",
    "                time.sleep(backoff_factor ** attempt)\n",
    "\n",
    "        logger.warning(f\"Google Books max retries exceeded for '{title}'.\")\n",
    "        return None  # Return None if all attempts are exhausted\n",
    "\n",
    "    def fetch_book_details(self, title, authors=None):\n",
    "        \"\"\"use Google Books API to retrieve missing information\"\"\"\n",
    "        details = self.fetch_book_details_open_library(title)\n",
    "        # logger.info(f\"Switching to Google Books API for '{title}' after Open Library retries exhausted.\")\n",
    "        # details = self.fetch_book_details_google_books(title, authors)\n",
    "        \n",
    "        return details or {\"title\": \"\", \"authors\": \"\", \"published_date\": \"\", \"description\": \"\"}\n",
    "\n",
    "    def fetch_details_for_part(self, books_part):\n",
    "        \"\"\"Fetch details for a subset of books.\"\"\"\n",
    "        results = []\n",
    "        for book in books_part:\n",
    "            data = self.fetch_book_details(book[\"Title\"], book.get(\"authors\"))\n",
    "            if data:\n",
    "                data[\"original_title\"] = book[\"Title\"]\n",
    "            results.append(data)\n",
    "        return results\n",
    "\n",
    "    def update_row_with_fetched_data(self, row, api_data):\n",
    "        \"\"\"Update only missing, null, or inconsistent values in a row based on API data.\"\"\"\n",
    "        updated_row = row.asDict()\n",
    "        \n",
    "        # Replace only if the original value is missing, null, or inconsistent\n",
    "        if not updated_row[\"Title\"]:\n",
    "            updated_row[\"Title\"] = api_data.get(\"title\", updated_row[\"Title\"])\n",
    "        if not updated_row[\"description\"]:\n",
    "            updated_row[\"description\"] = api_data.get(\"description\", updated_row[\"description\"])\n",
    "        if not updated_row[\"authors\"] or len(updated_row[\"authors\"].strip()) == 0:\n",
    "            updated_row[\"authors\"] = api_data.get(\"authors\", updated_row[\"authors\"])\n",
    "        if not updated_row[\"publisher\"]:\n",
    "            updated_row[\"publisher\"] = api_data.get(\"publisher\", updated_row[\"publisher\"])\n",
    "        \n",
    "        # Check for special characters or formats in `publishedDate`\n",
    "        if not updated_row[\"publishedDate\"] or \"?\" in updated_row[\"publishedDate\"] or \"*\" in updated_row[\"publishedDate\"] or len(updated_row[\"publishedDate\"]) <= 3:\n",
    "            updated_row[\"publishedDate\"] = api_data.get(\"published_date\", updated_row[\"publishedDate\"])\n",
    "\n",
    "        return updated_row\n",
    "\n",
    "\n",
    "    def fetch_book_details_open_library(self, title, authors=None, max_retries=3, backoff_factor=2):\n",
    "        \"\"\"Primary API: Fetch book details from Open Library API with mode=everything, retry, and backoff.\n",
    "           If title-only and simplified title searches fail, attempt search with both title and author.\"\"\"\n",
    "        \n",
    "        base_url = 'https://openlibrary.org'\n",
    "        search_url = f'{base_url}/search.json?q={title}&limit=1&mode=everything'\n",
    "        \n",
    "        def make_request(url):\n",
    "            \"\"\"Helper function to handle requests with retries and backoff.\"\"\"\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                else:\n",
    "                    logger.warning(f\"Received status code {response.status_code} for URL '{url}'\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Error fetching book details: {e}\")\n",
    "            return None\n",
    "        \n",
    "        def process_data(data, title):\n",
    "            \"\"\"Process Open Library data and return structured details.\"\"\"\n",
    "            if data and data.get(\"docs\"):\n",
    "                book_info = data[\"docs\"][0]\n",
    "                key = book_info.get(\"key\", \"\")\n",
    "                author_name = book_info.get(\"author_name\", \"\")\n",
    "                first_publish_year = str(book_info.get(\"first_publish_year\", \"\"))\n",
    "                \n",
    "                # Proceed with the second API call if 'key' exists\n",
    "                description = ''\n",
    "                first_publish_date = ''\n",
    "                if key:\n",
    "                    time.sleep(2)\n",
    "                    details_url = f\"{base_url}{key}.json\"\n",
    "                    details_data = make_request(details_url)\n",
    "                    if details_data:\n",
    "                        # Extract additional fields from the second API response\n",
    "                        description = details_data.get(\"description\", {}).get(\"value\", \"\") if isinstance(details_data.get(\"description\"), dict) else details_data.get(\"description\", \"\")\n",
    "                        first_publish_date = str(details_data.get(\"first_publish_date\", \"\"))\n",
    "                        \n",
    "                return {\n",
    "                    \"title\": str(title),\n",
    "                    \"authors\": author_name,\n",
    "                    \"published_date\": first_publish_year if first_publish_year else first_publish_date,\n",
    "                    \"description\": description if description else ''\n",
    "                }\n",
    "            return None\n",
    "        \n",
    "        # First attempt: search by the original title\n",
    "        time.sleep(3)\n",
    "        data = make_request(search_url)\n",
    "        result = process_data(data, title)\n",
    "        if result:\n",
    "            return result\n",
    "    \n",
    "        # Second attempt: search with both title and author if previous attempts failed\n",
    "        if authors:\n",
    "            simple_title = title.split(\"(\")[0].strip()\n",
    "            author_search_title = f\"{simple_title} {authors}\"\n",
    "            logger.info(f\"Both title and simplified title failed. Retrying with title and author '{author_search_title}'.\")\n",
    "            author_search_url = f'{base_url}/search.json?q={author_search_title}&limit=1&mode=everything'\n",
    "            time.sleep(2)\n",
    "            data = make_request(author_search_url)\n",
    "            result = process_data(data, author_search_title)\n",
    "            if result:\n",
    "                return result\n",
    "        \n",
    "        # Final attempt: search with a simplified title if the original title search failed\n",
    "        if \"(\" in title:\n",
    "            simple_title = title.split(\"(\")[0].strip()\n",
    "            logger.info(f\"No results for '{title}'. Retrying with simplified title '{simple_title}'.\")\n",
    "            simplified_url = f'{base_url}/search.json?q={simple_title}&limit=1&mode=everything'\n",
    "            time.sleep(3)\n",
    "            data = make_request(simplified_url)\n",
    "            result = process_data(data, simple_title)\n",
    "            if result:\n",
    "                return result\n",
    "        \n",
    "        logger.warning(f\"Open Library max retries exceeded for '{title}' and all fallback approaches.\")\n",
    "        return None  # Return None if all attempts are exhausted\n",
    "\n",
    "        \n",
    "    def fetch_missing_data_from_api(self):\n",
    "        \"\"\"Fetch missing data from APIs in parallel, updating only specific fields.\"\"\"\n",
    "        logger.info(\"Starting to fetch missing data from APIs with multithreading.\")\n",
    "        # Filter for rows with missing or inconsistent data\n",
    "        missing_df = self.df.filter(\n",
    "            (F.col(\"Title\").isNull()) | (F.col(\"description\").isNull()) | (F.col(\"authors\").isNull()) |\n",
    "            (F.col(\"publishedDate\").rlike(\"[?*]\")) | (F.col(\"publishedDate\").isNull()) | \n",
    "            (F.length(F.col(\"publishedDate\")) <= 3))\n",
    "        # Collect rows with missing data as a list of dictionaries\n",
    "        books_with_missing_data = [\n",
    "            {\"Title\": row[\"Title\"], \"authors\": row[\"authors\"], \"publishedDate\": row[\"publishedDate\"],\"description\":row[\"description\"]}\n",
    "            for row in missing_df.collect()\n",
    "        ]\n",
    "\n",
    "        # Divide the books into 10 parts for parallel processing\n",
    "        num_parts = 10\n",
    "        part_size = len(books_with_missing_data) // num_parts\n",
    "        books_parts = [books_with_missing_data[i:i + part_size] for i in range(0, len(books_with_missing_data), part_size)]\n",
    "\n",
    "        # Ensure exactly 10 parts by appending any remaining books to the last part\n",
    "        if len(books_parts) > num_parts:\n",
    "            books_parts[-2].extend(books_parts[-1])\n",
    "            books_parts = books_parts[:-1]\n",
    "\n",
    "        # # Process each part in a separate thread and collect results\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_parts) as executor:\n",
    "            future_to_part = {executor.submit(self.fetch_details_for_part, part): part for part in books_parts}\n",
    "            \n",
    "            all_results = []\n",
    "            for future in concurrent.futures.as_completed(future_to_part):\n",
    "                part_results = future.result()\n",
    "                all_results.extend(part_results)\n",
    "                logger.info(f\"Completed processing for a subset of books, collected {len(part_results)} results.\")\n",
    "\n",
    "        # Update rows selectively based on fetched data\n",
    "        updated_rows = []\n",
    "        for row in missing_df.collect():\n",
    "            api_data = next((res for res in all_results if res[\"original_title\"] == row[\"Title\"]), None)\n",
    "            if api_data:\n",
    "                # Update only specific fields\n",
    "                updated_row = self.update_row_with_fetched_data(row, api_data)\n",
    "                updated_rows.append(updated_row)\n",
    "\n",
    "        # Convert updated rows to DataFrame and merge\n",
    "        updated_df = spark.createDataFrame(updated_rows, schema=self.df.schema)\n",
    "        self.df = self.df.join(updated_df, on=\"Title\", how=\"left_anti\").union(updated_df)\n",
    "        self.df.write.csv(\"cleaned_data.csv\", index=False)\n",
    "        return self.df\n",
    "    \n",
    "    def feature_engineering(self, df_cleaned):\n",
    "        \"\"\"Performs feature engineering and assembles features into a single vector column.\"\"\"\n",
    "        logger.info(\"Starting feature engineering.\")\n",
    "\n",
    "        # Handle null values in columns that will be tokenized\n",
    "        df_cleaned = df_cleaned.withColumn(\"Title\", F.coalesce(F.col(\"Title\"), F.lit(\"\")))\n",
    "        df_cleaned = df_cleaned.withColumn(\"description\", F.coalesce(F.col(\"description\"), F.lit(\"\")))\n",
    "        df_cleaned = df_cleaned.withColumn(\"authors\", F.coalesce(F.col(\"authors\"), F.lit(\"\")))\n",
    "\n",
    "        # 1. TF-IDF for 'Title'\n",
    "        logger.info(\"Applying TF-IDF to 'Title'.\")\n",
    "        tokenizer_title = Tokenizer(inputCol=\"Title\", outputCol=\"title_tokens\")\n",
    "        df_cleaned = tokenizer_title.transform(df_cleaned)\n",
    "        hashing_tf_title = HashingTF(inputCol=\"title_tokens\", outputCol=\"title_tf\", numFeatures=1000)\n",
    "        df_cleaned = hashing_tf_title.transform(df_cleaned)\n",
    "        idf_title = IDF(inputCol=\"title_tf\", outputCol=\"title_tfidf\")\n",
    "        idf_model_title = idf_title.fit(df_cleaned)\n",
    "        df_cleaned = idf_model_title.transform(df_cleaned)\n",
    "\n",
    "        # 2. TF-IDF for 'Description'\n",
    "        logger.info(\"Applying TF-IDF to 'Description'.\")\n",
    "        tokenizer_desc = Tokenizer(inputCol=\"description\", outputCol=\"desc_tokens\")\n",
    "        df_cleaned = tokenizer_desc.transform(df_cleaned)\n",
    "        hashing_tf_desc = HashingTF(inputCol=\"desc_tokens\", outputCol=\"desc_tf\", numFeatures=1000)\n",
    "        df_cleaned = hashing_tf_desc.transform(df_cleaned)\n",
    "        idf_desc = IDF(inputCol=\"desc_tf\", outputCol=\"desc_tfidf\")\n",
    "        idf_model_desc = idf_desc.fit(df_cleaned)\n",
    "        df_cleaned = idf_model_desc.transform(df_cleaned)\n",
    "\n",
    "        # 3. TF-IDF for 'Authors'\n",
    "        logger.info(\"Applying TF-IDF to 'authors'.\")\n",
    "        tokenizer_authors = Tokenizer(inputCol=\"authors\", outputCol=\"authors_tokens\")\n",
    "        df_cleaned = tokenizer_authors.transform(df_cleaned)\n",
    "        hashing_tf_authors = HashingTF(inputCol=\"authors_tokens\", outputCol=\"authors_tf\", numFeatures=1000)\n",
    "        df_cleaned = hashing_tf_authors.transform(df_cleaned)\n",
    "        idf_authors = IDF(inputCol=\"authors_tf\", outputCol=\"authors_tfidf\")\n",
    "        idf_model_authors = idf_authors.fit(df_cleaned)\n",
    "        df_cleaned = idf_model_authors.transform(df_cleaned)\n",
    "\n",
    "        # 4. One-Hot Encoding for 'categories'\n",
    "        logger.info(\"Applying One-Hot Encoding to 'categories'.\")\n",
    "        indexer = StringIndexer(inputCol=\"categories\", outputCol=\"category_index\")\n",
    "        df_cleaned = indexer.fit(df_cleaned).transform(df_cleaned)\n",
    "        encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_onehot\")\n",
    "        df_cleaned = encoder.fit(df_cleaned).transform(df_cleaned)\n",
    "\n",
    "        # 5. Assemble all features into a single vector, excluding publishedDate_str\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=[\"title_tfidf\", \"desc_tfidf\", \"authors_tfidf\", \"category_onehot\"],\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "        df_with_features = assembler.transform(df_cleaned)\n",
    "\n",
    "        logger.info(\"Feature engineering completed.\")\n",
    "        return df_with_features\n",
    "\n",
    "    def cross_val_train_and_evaluate(self, df_cleaned):\n",
    "        \"\"\"Performs cross-validation, trains a linear regression model, and calculates MAPE.\"\"\"\n",
    "        logger.info(\"Starting cross-validation and model training.\")\n",
    "        df_with_features = self.feature_engineering(df_cleaned)\n",
    "        \n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"Impact\")\n",
    "        param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.01, 0.1, 0.5]).build()\n",
    "        \n",
    "        crossval = CrossValidator(estimator=lr,\n",
    "                                  estimatorParamMaps=param_grid,\n",
    "                                  evaluator=RegressionEvaluator(labelCol=\"Impact\", metricName=\"rmse\"),\n",
    "                                  numFolds=3)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        cv_model = crossval.fit(df_with_features)\n",
    "        total_training_time = time.time() - start_time\n",
    "        logger.info(f\"Model training completed in {total_training_time:.2f} seconds.\")\n",
    "        \n",
    "        evaluator_rmse = RegressionEvaluator(labelCol=\"Impact\", metricName=\"rmse\")\n",
    "        predictions = cv_model.transform(df_with_features)\n",
    "        rmse = evaluator_rmse.evaluate(predictions)\n",
    "        \n",
    "        mape = calculate_mape(predictions, label_col=\"Impact\", prediction_col=\"prediction\")\n",
    "        logger.info(f\"Cross-validation completed. RMSE: {rmse}, MAPE: {mape}\")\n",
    "        \n",
    "        return rmse, mape, total_training_time\n",
    "\n",
    "    def run_experiment(self, strategy):\n",
    "        \"\"\"Runs an experiment with a specified missing value treatment strategy and logs results to MLflow.\"\"\"\n",
    "        logger.info(f\"Starting experiment with missing value strategy: {strategy}\")\n",
    "        with mlflow.start_run(run_name=f\"Linear Regression with {strategy}\"):\n",
    "            df_cleaned = self.apply_missing_value_strategy(strategy)\n",
    "            original_row_count = self.df.count()\n",
    "            cleaned_row_count = df_cleaned.count()\n",
    "            mlflow.log_metric(\"original_row_count\", original_row_count)\n",
    "            mlflow.log_metric(\"cleaned_row_count\", cleaned_row_count)\n",
    "            \n",
    "            mlflow.log_param(\"missing_value_strategy\", strategy)\n",
    "            rmse, mape, total_training_time = self.cross_val_train_and_evaluate(df_cleaned)\n",
    "            \n",
    "            mlflow.log_metric(\"rmse\", rmse)\n",
    "            mlflow.log_metric(\"mape\", mape)\n",
    "            mlflow.log_metric(\"total_training_time\", total_training_time)\n",
    "            \n",
    "            cleaned_csv_path = f\"/tmp/cleaned_data_{strategy}.csv\"\n",
    "            df_cleaned.toPandas().to_csv(cleaned_csv_path, index=False)\n",
    "            mlflow.log_artifact(cleaned_csv_path)\n",
    "            \n",
    "            logger.info(f\"Experiment completed for strategy: {strategy}. RMSE: {rmse}, MAPE: {mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a61bfa-dfe7-4acf-a34b-d3d88446dc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/04 11:43:12 WARN Utils: Your hostname, Aditya resolves to a loopback address: 127.0.1.1; using 192.168.29.214 instead (on interface wlp4s0)\n",
      "24/11/04 11:43:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/04 11:43:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with optimized configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedSparkJob\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"8\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"512m\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:MaxGCPauseMillis=100\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "df = spark.read.format('CSV').options(header= True,\n",
    "                            delimiter = \",\",\n",
    "                            quote = '\"',\n",
    "                            escape = '\"',\n",
    "                            inferSchema = 'false',\n",
    "                            encoding = 'UTF8',\n",
    "                            multiline = True,\n",
    "                            rootTag = '',\n",
    "                            rowTag = '',\n",
    "                            attributePrefix = ''\n",
    "                            ).load(\"books_task.csv\")\n",
    "df = df.withColumn(\"Impact\", df[\"Impact\"].cast(FloatType()))\n",
    "# Define the regular expression to capture a four-digit year\n",
    "year_pattern = r\"(\\d{4})\"\n",
    "\n",
    "# Update the publishedDate column: if a year is found, keep it; otherwise, set to None\n",
    "df = df.withColumn(\n",
    "    \"publishedDate\",\n",
    "    when(\n",
    "        col(\"publishedDate\").rlike(year_pattern),\n",
    "        regexp_extract(col(\"publishedDate\"), year_pattern, 1)  # Extract the year if present\n",
    "    ).otherwise(None)  # Set to None if no valid year is found\n",
    ")\n",
    "df = df.drop('_c0')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db27c5f1-07e0-4fce-af1e-1e083da9fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "|Title|description|authors|publisher|publishedDate|categories|Impact|\n",
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "|    0|      12749|   2723|        0|          377|         0|     0|\n",
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f81dc81-274b-4bb4-96dc-5f5919bf6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format('CSV').options(header= True,\n",
    "                            delimiter = \",\",\n",
    "                            quote = '\"',\n",
    "                            escape = '\"',\n",
    "                            inferSchema = 'false',\n",
    "                            encoding = 'UTF8',\n",
    "                            multiline = True,\n",
    "                            rootTag = '',\n",
    "                            rowTag = '',\n",
    "                            attributePrefix = ''\n",
    "                            ).load(\"cleaned2.csv\")\n",
    "df1 = df1.withColumn(\"Impact\", df1[\"Impact\"].cast(FloatType()))\n",
    "df1 = df1.drop('_c0')\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e02b68-49d5-45ea-ac8a-d34340ce31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/04 11:43:22 INFO mlflow.tracking.fluent: Experiment with name 'Missing Value Treatment Experiments' does not exist. Creating a new experiment.\n",
      "INFO:__main__:Initialized MissingValueExperiment class.\n",
      "INFO:__main__:Starting experiment with missing value strategy: drop\n",
      "INFO:__main__:Applying missing value strategy: drop\n",
      "INFO:__main__:Starting cross-validation and model training.                     \n",
      "INFO:__main__:Starting feature engineering.\n",
      "INFO:__main__:Applying TF-IDF to 'Title'.\n",
      "INFO:__main__:Applying TF-IDF to 'Description'.                                 \n",
      "INFO:__main__:Applying TF-IDF to 'authors'.                                     \n",
      "INFO:__main__:Applying One-Hot Encoding to 'categories'.                        \n",
      "INFO:__main__:Feature engineering completed.                                    \n",
      "INFO:py4j.clientserver:Closing down clientserver connection                     \n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:__main__:Model training completed in 314.70 seconds.\n",
      "INFO:__main__:Cross-validation completed. RMSE: 62.529074276591466, MAPE: 6.069154413737776\n",
      "INFO:__main__:Experiment completed for strategy: drop. RMSE: 62.529074276591466, MAPE: 6.069154413737776\n",
      "INFO:__main__:Starting experiment with missing value strategy: fill_empty\n",
      "INFO:__main__:Applying missing value strategy: fill_empty\n",
      "INFO:__main__:Starting cross-validation and model training.                     \n",
      "INFO:__main__:Starting feature engineering.\n",
      "INFO:__main__:Applying TF-IDF to 'Title'.\n",
      "INFO:__main__:Applying TF-IDF to 'Description'.                                 \n",
      "INFO:__main__:Applying TF-IDF to 'authors'.                                     \n",
      "INFO:__main__:Applying One-Hot Encoding to 'categories'.                        \n",
      "INFO:__main__:Feature engineering completed.                                    \n",
      "INFO:py4j.clientserver:Closing down clientserver connection                     \n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:__main__:Model training completed in 314.28 seconds.\n",
      "INFO:__main__:Cross-validation completed. RMSE: 61.27186013492785, MAPE: 5.896272556812405\n",
      "INFO:__main__:Experiment completed for strategy: fill_empty. RMSE: 61.27186013492785, MAPE: 5.896272556812405\n",
      "INFO:__main__:Starting experiment with missing value strategy: fetch_api\n",
      "INFO:__main__:Applying missing value strategy: fetch_api\n",
      "INFO:__main__:Starting to fetch missing data from APIs with multithreading.\n",
      "INFO:__main__:Starting cross-validation and model training.                     \n",
      "INFO:__main__:Starting feature engineering.\n",
      "INFO:__main__:Applying TF-IDF to 'Title'.\n",
      "INFO:__main__:Applying TF-IDF to 'Description'.                                 \n",
      "INFO:__main__:Applying TF-IDF to 'authors'.                                     \n",
      "INFO:__main__:Applying One-Hot Encoding to 'categories'.                        \n",
      "INFO:__main__:Feature engineering completed.                                    \n",
      "INFO:py4j.clientserver:Closing down clientserver connection                     \n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:__main__:Model training completed in 320.37 seconds.\n",
      "INFO:__main__:Cross-validation completed. RMSE: 61.27186013492785, MAPE: 5.896272556812405\n",
      "INFO:__main__:Experiment completed for strategy: fetch_api. RMSE: 61.27186013492785, MAPE: 5.896272556812405\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and run experiments\n",
    "experiment = MissingValueExperiment(df)\n",
    "# for strategy in [\"fetch_api\"]:\n",
    "#     experiment.run_experiment(strategy)\n",
    "for strategy in [\"drop\", \"fill_empty\", \"fetch_api\"]:\n",
    "    experiment.run_experiment(strategy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
