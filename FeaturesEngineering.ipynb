{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tX0P0MKsZ76W",
    "outputId": "0ff451cc-3091-4e72-dc2d-de4f957c91bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- authors: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- publishedDate: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Spark NLP'\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import Tokenizer as NLPTokenizer\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, Tokenizer, HashingTF, IDF, PolynomialExpansion, StopWordsRemover\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F , SparkSession\n",
    "# Assemble the pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark.sql.functions import col, when, regexp_extract\n",
    "\n",
    "# Initialize Spark session with optimized configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=4\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.0\") \\\n",
    "    .config(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "df = spark.read.format('CSV').options(header= True,\n",
    "                            delimiter = \",\",\n",
    "                            quote = '\"',\n",
    "                            escape = '\"',\n",
    "                            inferSchema = 'false',\n",
    "                            encoding = 'UTF8',\n",
    "                            multiline = True,\n",
    "                            rootTag = '',\n",
    "                            rowTag = '',\n",
    "                            attributePrefix = ''\n",
    "                            ).load(\"cleaned2.csv\")\n",
    "\n",
    "df = df.withColumn(\"Impact\", df[\"Impact\"].cast(FloatType()))\n",
    "# df.write.parquet(\"cleaned1.parquet\")\n",
    "# df = spark.read.parquet('cleaned1.parquet')\n",
    "# Define the regular expression to capture a four-digit year\n",
    "year_pattern = r\"(\\d{4})\"\n",
    "\n",
    "# Update the publishedDate column: if a year is found, keep it; otherwise, set to None\n",
    "df = df.withColumn(\n",
    "    \"publishedDate\",\n",
    "    when(\n",
    "        col(\"publishedDate\").rlike(year_pattern),\n",
    "        regexp_extract(col(\"publishedDate\"), year_pattern, 1)  # Extract the year if present\n",
    "    )  # Set to None if no valid year is found\n",
    ")\n",
    "df = df.drop('_c0')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hElse9NtfAzu"
   },
   "outputs": [],
   "source": [
    "max_value = df.agg({\"Impact\": \"max\"}).collect()[0][0] + 1\n",
    "df = df.withColumn(\"Impact\", F.log(max_value - F.col(\"Impact\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPvfSfdFaMef",
    "outputId": "9d97f928-8718-4476-b69f-c66e6c8427d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "|Title|description|authors|publisher|publishedDate|categories|Impact|\n",
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "|    0|      10952|    456|        0|           35|         0|     0|\n",
      "+-----+-----------+-------+---------+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate null counts for all columns, handling both NaN and null values\n",
    "null_counts = df.select([\n",
    "    F.count(F.when(F.col(c).isNull() | (F.col(c) == \"\")|\n",
    "                   (F.col(c).cast(\"string\") == \"NaN\"), c)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "# Show null counts for verification\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRoF9u11z9Xb"
   },
   "outputs": [],
   "source": [
    "df = df.fillna({\"Title\": \"No Title\", \"description\": \"No Description\", \"authors\":\"Unknown author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCKUlbwZaVQi",
    "outputId": "06665b4f-d0ba-47ad-9026-b8162c17883f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|date_format| count|\n",
      "+-----------+------+\n",
      "|       NULL|    35|\n",
      "|       yyyy|138689|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the publishedDate column reference once to avoid repeated lookups\n",
    "published_date_col = F.col(\"publishedDate\")\n",
    "\n",
    "# Create the date_format column with optimized condition checks\n",
    "df = df.withColumn(\n",
    "    \"date_format\",\n",
    "    F.when(published_date_col.rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}[+-]\\d{2}:\\d{2}$\"), \"T-Timestamp\")\n",
    "    .when(published_date_col.rlike(r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z$\"), \"Z-Timestamp\")\n",
    "    .when(published_date_col.rlike(r\"^\\d{4}-\\d{2}-\\d{2}$\"), \"yyyy-dd-mm\")\n",
    "    .when(published_date_col.rlike(r\"^\\d{4}-\\d{2}$\"), \"yyyy-mm\")\n",
    "    .when(published_date_col.rlike(r\"^\\d{4}$\"), \"yyyy\")\n",
    "    .when(published_date_col.rlike(r\"^\\d{4}\\*$\"), \"yyyy*\")\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Perform the grouping and counting\n",
    "df.groupBy(\"date_format\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4O30_bZnaXri"
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"publishedYear\", year(col(\"publishedDate\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE9Azb1uaZSf"
   },
   "outputs": [],
   "source": [
    "median_year = df.approxQuantile(\"publishedYear\", [0.5], 0.01)[0]\n",
    "# Replace NULLs with the median year`\n",
    "df = df.fillna({\"publishedYear\": str(int(median_year))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0Cw3KTYaQiL"
   },
   "outputs": [],
   "source": [
    "# Compute book age using current year\n",
    "current_year = 2024\n",
    "df = df.withColumn(\"book_age\", lit(current_year) - col(\"publishedYear\"))\n",
    "\n",
    "# Create decade and century features\n",
    "df = df.withColumn(\"published_decade\", (col(\"publishedYear\") / 10).cast(\"int\") * 10)\n",
    "df = df.withColumn(\"published_century\", (col(\"publishedYear\") / 100).cast(\"int\") + 1)\n",
    "\n",
    "# Era categorization\n",
    "df = df.withColumn(\"published_era\",\n",
    "                   F.when(col(\"publishedYear\") >= 2000, \"Modern\")\n",
    "                   .when((col(\"publishedYear\") >= 1900) & (col(\"publishedYear\") < 2000), \"Contemporary\")\n",
    "                   .when((col(\"publishedYear\") >= 1800) & (col(\"publishedYear\") < 1900), \"Classic\")\n",
    "                   .otherwise(\"Ancient\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sSqMeIN1Jse"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = [\"date_format\",\"publishedDate\"]\n",
    "\n",
    "# Drop columns from the DataFrame\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-IDF for title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElE3F2107cEN"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF\n",
    "# Step 1: Apply HashingTF and IDF on Title column\n",
    "# Convert DataFrame column to RDD for HashingTF\n",
    "title_rdd = df.select(\"Title\").rdd.flatMap(lambda row: [row[0]])\n",
    "\n",
    "# Initialize HashingTF with a specific number of features\n",
    "hashingTF = HashingTF(numFeatures=20)\n",
    "title_tf = hashingTF.transform(title_rdd)\n",
    "\n",
    "# Cache the transformed RDD to speed up IDF computation\n",
    "title_tf.cache()\n",
    "\n",
    "# Apply IDF on the term frequency vectors\n",
    "idf_title = IDF().fit(title_tf)\n",
    "title_tfidf = idf_title.transform(title_tf)\n",
    "\n",
    "# Step 2: Apply HashingTF and IDF on description column\n",
    "# Convert description column to RDD\n",
    "desc_rdd = df.select(\"description\").rdd.flatMap(lambda row: [row[0]])\n",
    "\n",
    "# Transform description data\n",
    "desc_tf = hashingTF.transform(desc_rdd)\n",
    "desc_tf.cache()\n",
    "\n",
    "# Compute IDF and transform the term frequencies for description\n",
    "idf_desc = IDF().fit(desc_tf)\n",
    "desc_tfidf = idf_desc.transform(desc_tf)\n",
    "\n",
    "# Step 3: Convert RDDs back to DataFrames and join them with the original DataFrame\n",
    "# Convert title TF-IDF to DataFrame\n",
    "title_tfidf_df = title_tfidf.zipWithIndex().map(lambda x: (x[1], x[0])).toDF([\"id\", \"title_tfidf\"])\n",
    "\n",
    "# Convert description TF-IDF to DataFrame\n",
    "desc_tfidf_df = desc_tfidf.zipWithIndex().map(lambda x: (x[1], x[0])).toDF([\"id\", \"desc_tfidf\"])\n",
    "\n",
    "# Add an ID column to the original DataFrame to join on\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "# Join the original DataFrame with TF-IDF results\n",
    "df = df.join(title_tfidf_df, \"id\").join(desc_tfidf_df, \"id\").drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xiwpu4Ea1Baq",
    "outputId": "f183e681-9ee5-4e2f-c75a-783db9ac316d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = false)\n",
      " |-- description: string (nullable = false)\n",
      " |-- authors: string (nullable = false)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: double (nullable = true)\n",
      " |-- publishedYear: integer (nullable = true)\n",
      " |-- book_age: integer (nullable = true)\n",
      " |-- published_decade: integer (nullable = true)\n",
      " |-- published_century: integer (nullable = true)\n",
      " |-- published_era: string (nullable = false)\n",
      " |-- title_tfidf: vector (nullable = true)\n",
      " |-- desc_tfidf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Published Era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Op_Y1bl1Gvg"
   },
   "outputs": [],
   "source": [
    "# String indexer for era feature\n",
    "indexer = StringIndexer(inputCol=\"published_era\", outputCol=\"published_era_index\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "# One-hot encoding for indexed era feature\n",
    "encoder = OneHotEncoder(inputCol=\"published_era_index\", outputCol=\"published_era_encoded\")\n",
    "df = encoder.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for authors and publishers columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6TdbWZfakA2",
    "outputId": "a6cd23a8-53fe-4f3a-8f7e-0dd0961a1123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L10_128 download started this may take some time.\n",
      "Approximate size to download 21.9 MB\n",
      "[OK!]\n",
      "sent_small_bert_L10_128 download started this may take some time.\n",
      "Approximate size to download 21.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Frequency encoding for `authors` and `publisher` in a single operation\n",
    "author_publisher_counts = df.groupBy(\"authors\", \"publisher\").agg(\n",
    "    F.count(\"authors\").alias(\"author_frequency\"),\n",
    "    F.count(\"publisher\").alias(\"publisher_frequency\")\n",
    ")\n",
    "\n",
    "# Join frequency columns to the original DataFrame\n",
    "df = df.join(author_publisher_counts, on=[\"authors\", \"publisher\"], how=\"left\")\n",
    "\n",
    "# Document Assemblers for Authors and Publishers\n",
    "document_assembler_authors = DocumentAssembler().setInputCol(\"authors\").setOutputCol(\"authors_document\")\n",
    "\n",
    "document_assembler_publishers = DocumentAssembler() \\\n",
    "    .setInputCol(\"publisher\") \\\n",
    "    .setOutputCol(\"publishers_document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "# BERT Embeddings for Authors and Publishers\n",
    "bert_embeddings_authors = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L10_128\", \"en\") \\\n",
    "    .setInputCols([\"authors_document\"]) \\\n",
    "    .setOutputCol(\"authors_embedding\")\n",
    "\n",
    "bert_embeddings_publishers = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L10_128\", \"en\") \\\n",
    "    .setInputCols([\"publishers_document\"]) \\\n",
    "    .setOutputCol(\"publishers_embedding\")\n",
    "\n",
    "# Define Pipeline for Document Assembly and BERT Embeddings\n",
    "embedding_pipeline = Pipeline(stages=[\n",
    "    document_assembler_authors,\n",
    "    document_assembler_publishers,\n",
    "    bert_embeddings_authors,\n",
    "    bert_embeddings_publishers\n",
    "])\n",
    "\n",
    "# Apply the Pipeline to obtain embeddings\n",
    "df = embedding_pipeline.fit(df).transform(df)\n",
    "df = df.withColumn(\n",
    "    \"author_publisher_combined_embedding\",\n",
    "    F.concat(F.col(\"authors_embedding.embeddings\"), F.col(\"publishers_embedding.embeddings\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9yoYEOFmr8Q",
    "outputId": "d3eba52b-89ab-450d-e5fd-fe140761b674"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Title: string, description: string, categories: string, Impact: double, publishedYear: int, book_age: int, published_decade: int, published_century: int, published_era: string, title_tfidf: vector, desc_tfidf: vector, published_era_index: double, published_era_encoded: vector, author_frequency: bigint, publisher_frequency: bigint, authors_document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, publishers_document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, authors_embedding: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, publishers_embedding: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, author_publisher_combined_embedding: array<array<float>>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"authors\",\"publisher\"]\n",
    "df.drop(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4D0T98YHrn-P"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "# Using FeatureHasher for categories\n",
    "hasher = FeatureHasher(inputCols=[\"categories\"], outputCol=\"Category_Index\", numFeatures=20)\n",
    "df = hasher.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Title and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ah6-i0RQa0Tb",
    "outputId": "615fd4e3-b330-4a3c-de67-4161b51a33d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[OK!]\n",
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Document Assemblers for Title and Description\n",
    "document_title = DocumentAssembler() \\\n",
    "    .setInputCol(\"Title\") \\\n",
    "    .setOutputCol(\"title_document\")\n",
    "\n",
    "document_desc = DocumentAssembler() \\\n",
    "    .setInputCol(\"description\") \\\n",
    "    .setOutputCol(\"desc_document\")\n",
    "\n",
    "# Tokenizers for Title and Description\n",
    "token_title = NLPTokenizer() \\\n",
    "    .setInputCols([\"title_document\"]) \\\n",
    "    .setOutputCol(\"title_token\")\n",
    "\n",
    "token_desc = NLPTokenizer() \\\n",
    "    .setInputCols([\"desc_document\"]) \\\n",
    "    .setOutputCol(\"desc_token\")\n",
    "\n",
    "# Normalizers for Title and Description\n",
    "normalizer_title = Normalizer() \\\n",
    "    .setInputCols([\"title_token\"]) \\\n",
    "    .setOutputCol(\"title_normal\")\n",
    "\n",
    "normalizer_desc = Normalizer() \\\n",
    "    .setInputCols([\"desc_token\"]) \\\n",
    "    .setOutputCol(\"desc_normal\")\n",
    "\n",
    "# Vivekn Sentiment Models for Title and Description\n",
    "vivekn_title = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"title_document\", \"title_normal\"]) \\\n",
    "    .setOutputCol(\"t_sentiment\")\n",
    "\n",
    "vivekn_desc = ViveknSentimentModel.pretrained() \\\n",
    "    .setInputCols([\"desc_document\", \"desc_normal\"]) \\\n",
    "    .setOutputCol(\"d_sentiment\")\n",
    "\n",
    "# Finishers to Extract Final Sentiment for Title and Description\n",
    "finisher_title = Finisher() \\\n",
    "    .setInputCols([\"t_sentiment\"]) \\\n",
    "    .setOutputCols([\"title_sentiment\"])\n",
    "\n",
    "finisher_desc = Finisher() \\\n",
    "    .setInputCols([\"d_sentiment\"]) \\\n",
    "    .setOutputCols([\"description_sentiment\"])\n",
    "\n",
    "# Assemble the Sentiment Analysis Pipeline\n",
    "sentiment_pipeline = Pipeline().setStages([\n",
    "    document_title, token_title, normalizer_title, vivekn_title, finisher_title,\n",
    "    document_desc, token_desc, normalizer_desc, vivekn_desc, finisher_desc\n",
    "])\n",
    "\n",
    "# Fit and Transform the DataFrame for Sentiment Analysis\n",
    "df = sentiment_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Extract the first sentiment value from the array (assuming only one sentiment per array)\n",
    "df = df.withColumn(\"title_sentiment_value\", F.element_at(\"title_sentiment\", 1))\n",
    "df = df.withColumn(\"description_sentiment_value\", F.element_at(\"description_sentiment\", 1))\n",
    "\n",
    "# Assign values directly based on sentiment without StringIndexer\n",
    "df = df.withColumn(\"title_sentiment_encoded\",\n",
    "                   F.when(F.col(\"title_sentiment_value\") == \"positive\", 1)\n",
    "                   .when(F.col(\"title_sentiment_value\") == \"negative\", 2)\n",
    "                   .otherwise(0))\n",
    "\n",
    "df = df.withColumn(\"description_sentiment_encoded\",\n",
    "                   F.when(F.col(\"description_sentiment_value\") == \"positive\", 1)\n",
    "                   .when(F.col(\"description_sentiment_value\") == \"negative\", 2)\n",
    "                   .otherwise(0))\n",
    "\n",
    "# Drop intermediate columns if desired\n",
    "df = df.drop(\"title_sentiment\", \"description_sentiment\", \"title_sentiment_value\", \"description_sentiment_value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rnQn4Vpvdgc",
    "outputId": "3decfc10-3426-4917-b2f8-9521a5126ebe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['authors',\n",
       " 'publisher',\n",
       " 'Title',\n",
       " 'description',\n",
       " 'categories',\n",
       " 'Impact',\n",
       " 'publishedYear',\n",
       " 'book_age',\n",
       " 'published_decade',\n",
       " 'published_century',\n",
       " 'published_era',\n",
       " 'title_tfidf',\n",
       " 'desc_tfidf',\n",
       " 'published_era_index',\n",
       " 'published_era_encoded',\n",
       " 'author_frequency',\n",
       " 'publisher_frequency',\n",
       " 'author_publisher_combined_embedding',\n",
       " 'Category_Index',\n",
       " 'title_sentiment_encoded',\n",
       " 'description_sentiment_encoded']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sthEhOS6wZd"
   },
   "outputs": [],
   "source": [
    "# Define the columns to drop based on analysis of their redundancy and relevance\n",
    "columns_to_drop = [\n",
    "    # Original categorical/text columns\n",
    "    \"authors\", \"publisher\", \"categories\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2FLiLE0z3fAr",
    "outputId": "a5ead228-3645-413d-87c3-87ed7c514fd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = false)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- Title: string (nullable = false)\n",
      " |-- description: string (nullable = false)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: double (nullable = true)\n",
      " |-- publishedYear: integer (nullable = true)\n",
      " |-- book_age: integer (nullable = true)\n",
      " |-- published_decade: integer (nullable = true)\n",
      " |-- published_century: integer (nullable = true)\n",
      " |-- published_era: string (nullable = false)\n",
      " |-- title_tfidf: vector (nullable = true)\n",
      " |-- desc_tfidf: vector (nullable = true)\n",
      " |-- published_era_index: double (nullable = false)\n",
      " |-- published_era_encoded: vector (nullable = true)\n",
      " |-- author_frequency: long (nullable = true)\n",
      " |-- publisher_frequency: long (nullable = true)\n",
      " |-- author_publisher_combined_embedding: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = false)\n",
      " |-- Category_Index: vector (nullable = true)\n",
      " |-- title_sentiment_encoded: integer (nullable = false)\n",
      " |-- description_sentiment_encoded: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uabpjKaf-i-B"
   },
   "outputs": [],
   "source": [
    "def array_to_dense_vector(array):\n",
    "    return Vectors.dense(array) if array else Vectors.dense([])\n",
    "array_to_vector_udf = F.udf(array_to_dense_vector, VectorUDT())\n",
    "df = df.withColumn(\"author_publisher_combined_embedding\", array_to_vector_udf(F.flatten(\"author_publisher_combined_embedding\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fw5hFNBhECH_",
    "outputId": "b887d1b3-bc79-4764-ac19-f11c2476a14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = false)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- Title: string (nullable = false)\n",
      " |-- description: string (nullable = false)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: double (nullable = true)\n",
      " |-- publishedYear: integer (nullable = true)\n",
      " |-- book_age: integer (nullable = true)\n",
      " |-- published_decade: integer (nullable = true)\n",
      " |-- published_century: integer (nullable = true)\n",
      " |-- published_era: string (nullable = false)\n",
      " |-- title_tfidf: vector (nullable = true)\n",
      " |-- desc_tfidf: vector (nullable = true)\n",
      " |-- published_era_index: double (nullable = false)\n",
      " |-- published_era_encoded: vector (nullable = true)\n",
      " |-- author_frequency: long (nullable = true)\n",
      " |-- publisher_frequency: long (nullable = true)\n",
      " |-- author_publisher_combined_embedding: vector (nullable = true)\n",
      " |-- Category_Index: vector (nullable = true)\n",
      " |-- title_sentiment_encoded: integer (nullable = false)\n",
      " |-- description_sentiment_encoded: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\n",
    "    \"authors\",\n",
    "    \"publisher\",\n",
    "    \"published_era\",\n",
    "    \"categories\"\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_cleaned = df.drop(*columns_to_drop)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP9Viy0xCUOP"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "# Convert SparseVector to DenseVector for TF-IDF columns\n",
    "# Convert SparseVector to DenseVector for TF-IDF columns\n",
    "dense_vector_udf = F.udf(lambda v: Vectors.dense(v.toArray()) if v is not None else None, VectorUDT())\n",
    "\n",
    "df = df.withColumn(\"title_tfidf_dense\", dense_vector_udf(F.col(\"title_tfidf\")))\n",
    "df = df.withColumn(\"desc_tfidf_dense\", dense_vector_udf(F.col(\"desc_tfidf\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZFeZwf1DJIi",
    "outputId": "e6ba6e94-5b64-42cf-8784-392b9d54c4ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = false)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- Title: string (nullable = false)\n",
      " |-- description: string (nullable = false)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: double (nullable = true)\n",
      " |-- publishedYear: integer (nullable = true)\n",
      " |-- book_age: integer (nullable = true)\n",
      " |-- published_decade: integer (nullable = true)\n",
      " |-- published_century: integer (nullable = true)\n",
      " |-- published_era: string (nullable = false)\n",
      " |-- published_era_index: double (nullable = false)\n",
      " |-- published_era_encoded: vector (nullable = true)\n",
      " |-- author_frequency: long (nullable = true)\n",
      " |-- publisher_frequency: long (nullable = true)\n",
      " |-- author_publisher_combined_embedding: vector (nullable = true)\n",
      " |-- Category_Index: vector (nullable = true)\n",
      " |-- title_sentiment_encoded: integer (nullable = false)\n",
      " |-- description_sentiment_encoded: integer (nullable = false)\n",
      " |-- title_tfidf_dense: vector (nullable = true)\n",
      " |-- desc_tfidf_dense: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"title_tfidf\", \"desc_tfidf\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMZDxtcKbAF6",
    "outputId": "709f9c3a-0bae-4733-9ff9-c9a0c8c8d815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- authors: string (nullable = false)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- Title: string (nullable = false)\n",
      " |-- description: string (nullable = false)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- Impact: double (nullable = true)\n",
      " |-- publishedYear: integer (nullable = true)\n",
      " |-- book_age: integer (nullable = true)\n",
      " |-- published_decade: integer (nullable = true)\n",
      " |-- published_century: integer (nullable = true)\n",
      " |-- published_era: string (nullable = false)\n",
      " |-- published_era_index: double (nullable = false)\n",
      " |-- published_era_encoded: vector (nullable = true)\n",
      " |-- author_frequency: long (nullable = true)\n",
      " |-- publisher_frequency: long (nullable = true)\n",
      " |-- author_publisher_combined_embedding: vector (nullable = true)\n",
      " |-- Category_Index: vector (nullable = true)\n",
      " |-- title_sentiment_encoded: integer (nullable = false)\n",
      " |-- description_sentiment_encoded: integer (nullable = false)\n",
      " |-- title_tfidf_dense: vector (nullable = true)\n",
      " |-- desc_tfidf_dense: vector (nullable = true)\n",
      " |-- assembled_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define final feature columns\n",
    "feature_columns = [\n",
    "    \"publishedYear\",\n",
    "    \"book_age\",\n",
    "    \"published_decade\",\n",
    "    \"published_century\",\n",
    "    \"author_frequency\",\n",
    "    \"publisher_frequency\",\n",
    "    \"published_era_encoded\",\n",
    "    \"Category_Index\",\n",
    "    \"title_sentiment_encoded\",\n",
    "    \"description_sentiment_encoded\",\n",
    "    \"title_tfidf_dense\",\n",
    "    \"desc_tfidf_dense\",\n",
    "    \"author_publisher_combined_embedding\"\n",
    "]\n",
    "\n",
    "# Filter out any columns that do not exist in the DataFrame\n",
    "available_feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "\n",
    "# # Assemble all available features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=available_feature_columns, outputCol=\"assembled_features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "# df_assembled.write.parquet(\"df_assembled.parquet\")\n",
    "# Verify the schema and data types\n",
    "df_assembled.printSchema()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
